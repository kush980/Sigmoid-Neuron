# -*- coding: utf-8 -*-
"""SigmoidNeurnon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sKMH6TcXuAAbgXtXYT0oMgWQjGXYtcNv

#Plotting Sigmoid Neuron
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score,mean_squared_error
from tqdm import tqdm_notebook

"""$S_{w,b}(x)=\frac{1}{1+e^{-(wx+b)}}$"""

def sigmoid(x,w,b):
  return 1/(1+np.exp(-(w*x+b)))

sigmoid(1,0.5,0)

w=0.8     #@param {type: "slider", min:-2, max:2,step:0.1}
b=0.1     #@param {type: "slider", min:-2, max:2,step:0.1}
X=np.linspace(-10,10,100)
Y=sigmoid(X,w,b)

type(X)

plt.plot(X,Y)
plt.show()

"""$S_{w_1,w_2,b}(x_1,x_2)=\frac{1}{1+e^{-(w_1x_1+w_2x_2+b)}}$"""

def sigmoid_2d(x1,x2,w1,w2,b):
  return 1/(1+np.exp(-(w1*x1+w2*x2+b)))

sigmoid_2d(1,0,0.5,0,0)

#For plotting 3D Figure
from mpl_toolkits import mplot3d

X1=np.linspace(-10,10,100)
X2=np.linspace(-10,10,100)
XX1,XX2=np.meshgrid(X1,X2)

print(X1.shape,X2.shape,XX1.shape,XX2.shape)

w1 = 2
w2 = -0.5
b = 0
Y = sigmoid_2d(XX1,XX2,w1,w2,b)

import matplotlib.colors
my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list("",["red","yellow","green"])

plt.contourf(XX1,XX2,Y,cmap=my_cmap,alpha=0.7)
plt.show()

##Plotting a 3D figure
fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(XX1,XX2,Y, cmap='viridis')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y');

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(XX1,XX2,Y, cmap='viridis')
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('y');

#To view it from other angles
ax.view_init(30,180)

"""#Loss for a Given Dataset"""

w_unk=0.5
b_unk=0.25

X=np.random.random(25)*20-10
Y=sigmoid(X,w_unk,b_unk)

plt.plot(X,Y,'*')
plt.show()

def loss_fun(X,Y,w_est,b_est):
  loss=0
  for x,y in zip(X,Y):
    loss+=(y-sigmoid(x,w_est,b_est))**2
  return loss

W=np.linspace(0,2,101)
B=np.linspace(-1,1,101)

WW,BB=np.meshgrid(W,B)
loss = np.zeros(WW.shape)

for i in range(WW.shape[0]):
  for j in range(WW.shape[1]):
    loss[i,j]= loss_fun(X,Y,WW[i,j],BB[i,j])

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_surface(WW,BB,loss, cmap='viridis')
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('loss');

ax.view_init(30,270)

ij = np.argmin(loss)
i = int(np.floor(ij/loss.shape[1]))
j = ij - i*loss.shape[1]

print(i,j)

#Points where LOSS is MINIMUM
print(WW[i,j],BB[i,j])

"""#Sigmoid Neuron Class"""

class SigmoidNeuron:
  def __init__(self):
    self.w=0
    self.b=0
  
  def perceptron(self,x):
    return np.dot(x,self.w.T)+self.b

  def sigmoid(self,x):
    return 1.0/(1.0+np.exp(-x))
  
  def grad_w(self,x,y):
    y_pred=self.sigmoid(self.perceptron(x))
    return (y_pred-y)*y_pred*(1-y_pred)*x

  def grad_b(self,x,y):
    y_pred=self.sigmoid(self.perceptron(x))
    return (y_pred-y)*y_pred*(1-y_pred)
  
  def fit(self,X,Y,epochs=1,lr=1,initialise=True,display_loss=False):
    if initialise:
      self.w=np.random.randn(1,X.shape[1])
      self.b=0
    if display_loss:
      loss={}
    
    for i in tqdm_notebook(range(epochs),total=epochs,unit="epoch"):
      db=0
      dw=0
      for x,y in zip(X,Y):
        dw+=self.grad_w(x,y)
        db+=self.grad_b(x,y)
      self.w-=lr*dw
      self.b-=lr*db
      
      if display_loss:
        Y_pred = self.sigmoid(self.perceptron(X))
        loss[i] = mean_squared_error(Y_pred,Y)
    
    if display_loss:
      plt.plot(list(loss.values()))
      plt.xlabel('Epochs')
      plt.ylabel('Mean squared Error')
      plt.show()
  def predict(self,X):
    Y_pred=[]
    for x in X:
      y_pred = self.sigmoid(self.perceptron(x))
      Y_pred.append(y_pred)
    return np.array(Y_pred)

X=np.asarray([[2.5,2.5],[4,-1],[1,-4],[-3,1.25],[-2,-4],[1,5]])
Y=[1,1,1,0,0,0]

sn=SigmoidNeuron()
sn.fit(X,Y,1,0.25,True)

def plot_sn(X,Y,sn,ax):
  X1=np.linspace(-10,10,100)
  X2=np.linspace(-10,10,100)
  XX1,XX2 = np.meshgrid(X1,X2)
  YY = np.zeros(XX1.shape)
  for i in range(X2.size):
    for j in range(X1.size):
      val = np.asarray([X1[j],X2[i]])
      YY[i,j]=sn.sigmoid(sn.perceptron(val))
  ax.contourf(XX1,XX2,YY,cmap=my_cmap,alpha=0.7)
  ax.scatter(X[:,0],X[:,1],c=Y)
  ax.plot()

sn.fit(X,Y,1,0.75,True)
N=20
plt.figure(figsize=(10,N*5))
for i in range(N):
  print(sn.w,sn.b)
  ax = plt.subplot(N,1,i+1)
  plot_sn(X,Y,sn,ax)
  sn.fit(X,Y,1,0.75,False)

"""#Load Dataset"""

!wget https://drive.google.com/file/d/1dfoXGWVOeKJIWIn2LSEKrN-ovccNjAx9/view?usp=sharing

!ls

data=pd.read_csv('mobile_cleaned (1).csv')

data.head()

data.shape

X=data.drop('Rating',axis=1)

Y=data['Rating'].values

Y

threshold = 4.2
data['Class'] = (data['Rating']>=threshold).astype(np.int)

data['Class'].value_counts(normalize=True)

Y_binary = data['Class'].values

Y_binary

"""##Standardisation"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

R = np.random.random([100,1])

plt.plot(R)
plt.show()

np.mean(R)

np.std(R)

scaler.fit(R)

RT = scaler.transform(R)

plt.plot(RT)
plt.show()

np.mean(RT)

np.std(RT)

X_train , X_test , Y_train , Y_test = train_test_split(X,Y,random_state=0,stratify=Y_binary)

X_train.shape , X_test.shape

X_scaled_Train = scaler.fit_transform(X_train)
X_scaled_Test = scaler.transform(X_test)

min_max_scaler = MinMaxScaler()

Y_scaled_Train = min_max_scaler.fit_transform(Y_train.reshape(-1,1))
Y_scaled_Test = min_max_scaler.transform(Y_test.reshape(-1,1))

Y_scaled_Train

scaled_threshold = list(min_max_scaler.transform(np.array([threshold]).reshape(1,-1)))[0][0]

scaled_threshold

Y_binary_Train = (Y_scaled_Train>scaled_threshold).astype(np.int).ravel()

Y_binary_Test = (Y_scaled_Test>scaled_threshold).astype(np.int).ravel()





"""#Train Real Data"""

sn = SigmoidNeuron()

sn.fit(X_scaled_Train,Y_scaled_Train,epochs=5000,lr=0.01,display_loss=True)

Y_pred_train = sn.predict(X_scaled_Train)
Y_pred_test = sn.predict(X_scaled_Test)

Y_pred_binary_train = (Y_pred_train>scaled_threshold).astype(np.int).ravel()
Y_pred_binary_test = (Y_pred_test>scaled_threshold).astype(np.int).ravel()

accuracy_train = accuracy_score(Y_pred_binary_train,Y_binary_Train)
accuracy_test = accuracy_score(Y_pred_binary_test,Y_binary_Test)

print(accuracy_train , accuracy_test)



"""#Sigmoid class with cross entropy loss Function"""

class SigmoidNeuron:
  def __init__(self):
    self.w=0
    self.b=0
  
  def perceptron(self,x):
    return np.dot(x,self.w.T)+self.b

  def sigmoid(self,x):
    return 1.0/(1.0+np.exp(-x))
  
  def grad_w(self,x,y):
    y_pred=self.sigmoid(self.perceptron(x))
    return (y_pred-y)*x

  def grad_b(self,x,y):
    y_pred=self.sigmoid(self.perceptron(x))
    return (y_pred-y)
  
  def fit(self,X,Y,epochs=1,lr=1,initialise=True,display_loss=False):
    if initialise:
      self.w=np.random.randn(1,X.shape[1])
      self.b=0
    if display_loss:
      loss={}
    
    for i in tqdm_notebook(range(epochs),total=epochs,unit="epoch"):
      db=0
      dw=0
      for x,y in zip(X,Y):
        dw+=self.grad_w(x,y)
        db+=self.grad_b(x,y)
      self.w-=lr*dw
      self.b-=lr*db
      
      if display_loss:
        Y_pred = self.sigmoid(self.perceptron(X))
        loss[i] = mean_squared_error(Y_pred,Y)
    
    if display_loss:
      plt.plot(list(loss.values()))
      plt.xlabel('Epochs')
      plt.ylabel('Mean squared Error')
      plt.show()
  def predict(self,X):
    Y_pred=[]
    for x in X:
      y_pred = self.sigmoid(self.perceptron(x))
      Y_pred.append(y_pred)
    return np.array(Y_pred)

